{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract sentences and their corresponding trigger tag list from pickle files\n",
    "#import bengali_word_list\n",
    "import pickle\n",
    "file=open('../work_space/august_last/bengali_pickle_latest/bengali_training_and_vocab_for_trigger_22_8.pkl','rb')\n",
    "train_x=pickle.load(file)\n",
    "train_y=pickle.load(file)\n",
    "test_x=pickle.load(file)\n",
    "test_y=pickle.load(file)\n",
    "vocab=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[36713, 36713, 36713, 0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])\n",
    "print(train_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1687\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for tag in test_y:\n",
    "    if str(tag)!='None':\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36714\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vocab=[]\n",
    "embedding_vector_list = []\n",
    "file = open('../work_space/august_last/bengali_pickle_latest/bengali_word_vector_22_8','r')#extract word vectors from fastext\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab_word = row[0]\n",
    "    glove_vocab.append(vocab_word)\n",
    "    embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "    embedding_vector_list.append(embed_vector)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36714"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_matrix = np.asarray(embedding_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36714, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_x=[]\n",
    "train_data_y=[]\n",
    "test_data_x=[]\n",
    "test_data_y=[]\n",
    "train_data_x= train_x\n",
    "train_data_y= train_y\n",
    "test_data_x= test_x\n",
    "test_data_y= test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36713, 36713, 71, 376, 645, 645, 660, 113, 661]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_total=train_y+test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303212\n"
     ]
    }
   ],
   "source": [
    "print(len(y_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checking unique event types\n",
    "\n",
    "uniq_event_type=[]\n",
    "for k in range(len(y_total)):\n",
    "    if y_total[k] not in uniq_event_type:\n",
    "        uniq_event_type.append(y_total[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[None, u'NORMAL_BOMBING', u'SUICIDE_ATTACK', u'CYCLONE', u'VEHICULAR_COLLISION', u'EARTHQUAKE', u'SHOOT_OUT', u'TERRORIST_ATTACK', u'SURGICAL_STRIKES', u'RIOTS', u'FIRE', u'ARMED_CONFLICTS', u'TSUNAMI', u'TRANSPORT_HAZARDS', u'EPIDEMIC', u'COLD_WAVE', u'PANDEMIC', u'LAND_SLIDE', u'STORM', u'FLOODS', u'DROUGHT', u'AVIATION_HAZARD', u'FOREST_FIRE', u'INDUSTRIAL_ACCIDENT', u'SEISMIC_RISK', u'HEAT_WAVE', u'TRAIN_COLLISION', u'VOLCANO', u'BLIZZARD', u'AVALANCHES', u'HURRICANE', u'TORNADO']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniq_event_type))\n",
    "print(uniq_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uniq_event_type[0]=0#to avoide None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train event types are transformed to numeric value\n",
    "\n",
    "k=0\n",
    "train_data_y_numeric=[]\n",
    "for k in range(len(train_data_y)):\n",
    "    train_data_y_numeric.append(uniq_event_type.index(train_data_y[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cfbe0fd97745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data_y_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest_data_y_numeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniq_event_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_y' is not defined"
     ]
    }
   ],
   "source": [
    "#test event types are transformed to numeric value\n",
    "'''\n",
    "k=0\n",
    "test_data_y_numeric=[]\n",
    "for k in range(len(test_data_y)):\n",
    "    test_data_y_numeric.append(uniq_event_type.index(test_data_y[k]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_hot_train=[]\n",
    "for y_ in train_data_y_numeric:\n",
    "    y_hot_train.append(np.eye(len(uniq_event_type))[y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file=open('../work_space/august_last/bengali_pickle_latest/word_list.pkl','rb')\n",
    "word_list_train=pickle.load(file)\n",
    "word_list_test=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254888\n",
      "48324\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list_train))\n",
    "print(len(word_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=train_data_x\n",
    "y_train=y_hot_train\n",
    "#x_test=test_data_x\n",
    "#y_test=y_hot_test\n",
    "#y_test_actual=test_data_y_numeric\n",
    "#x_dev=train_data_x[210000:]\n",
    "#y_dev=y_hot_train[210000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs/16CS91R02/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 9), dtype=int32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 32), dtype=int32)\n",
      "<tf.Variable 'Variable:0' shape=(400, 32) dtype=float32_ref>\n",
      "[<tf.Tensor 'split:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:1' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:2' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:3' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:4' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:5' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:6' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:7' shape=(?, 300) dtype=float32>, <tf.Tensor 'split:8' shape=(?, 300) dtype=float32>]\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 9, 300, 1), dtype=float32)\n",
      "Tensor(\"dropout/dropout/mul:0\", shape=(?, 1, 1, 200), dtype=float32)\n",
      "Tensor(\"dropout_1/dropout/mul:0\", shape=(?, 1, 1, 200), dtype=float32)\n",
      "Tensor(\"dropout_2/dropout/mul:0\", shape=(?, 1, 1, 200), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 1, 1, 200), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(?, 600), dtype=float32)\n",
      "Tensor(\"concat_5:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"concat_10:0\", shape=(?, 1000), dtype=float32)\n",
      "Tensor(\"add_3:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"y_pred:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"ArgMax_2:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_length=9\n",
    "embed_length=300\n",
    "no_class=len(uniq_event_type)\n",
    "no_hidden=200\n",
    "learning_rate=0.001\n",
    "tf.reset_default_graph()\n",
    "x=tf.placeholder(tf.int32,[None,input_length])\n",
    "print(x)\n",
    "y=tf.placeholder(tf.int32,[None,no_class])\n",
    "print(y)\n",
    "\n",
    "w=tf.Variable(tf.random_normal([2*no_hidden,no_class],stddev=0.35,name='weight_out'),trainable=True)\n",
    "print(w)\n",
    "#w_embedding=tf.Variable(tf.random_normal([len(vocab),embed_length],stddev=0.35,name='weight_embed'),trainable=True)\n",
    "#print(w_embedding)\n",
    "#number_of_layers=3\n",
    "\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    w_embedding = tf.Variable(tf.constant(0.0, shape=[len(vocab), embed_length]), trainable=True, name=\"w_embedding\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [len(vocab), embed_length])\n",
    "    embedding_init = w_embedding.assign(embedding_placeholder)\n",
    "    embedded_chars = tf.nn.embedding_lookup(w_embedding,x)\n",
    "\n",
    "#embedded_chars = tf.nn.embedding_lookup(w_embedding,x)\n",
    "\n",
    "x_rnn=tf.transpose(embedded_chars,[1,0,2])\n",
    "x_rnn_shape=tf.reshape(x_rnn,[-1,embed_length])\n",
    "x_shape_rnn=tf.split(x_rnn_shape,input_length,0)\n",
    "print(x_shape_rnn)\n",
    "\n",
    "x_shape_conv=tf.reshape(embedded_chars,[-1,input_length,embed_length,1])\n",
    "print(x_shape_conv)\n",
    "\n",
    "filter_size1=2\n",
    "filter_size2=3\n",
    "filter_size3=4\n",
    "no_of_filter=200\n",
    "\n",
    "def create_wt(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_bt(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "def create_conv(ip, no_of_filter, conv_filter_size, no_of_channel):\n",
    "    wt = create_wt(shape=[conv_filter_size,embed_length,1,no_of_filter])\n",
    "    b = create_bt(size = no_of_filter)\n",
    "    layer = tf.nn.conv2d(input = ip, filter= wt, strides =[1,1,1,1], padding='VALID')\n",
    "    layer_conv=tf.nn.relu(layer+b)\n",
    "    layer=tf.nn.max_pool(value= layer_conv, ksize= [1,input_length-conv_filter_size+1,1,1],strides=[1,1,1,1],padding='VALID' )\n",
    "    layer = tf.layers.dropout(layer, rate=0.5, training=True)\n",
    "    print(layer)\n",
    "    return layer\n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    layer=tf.reshape(layer,[-1,no_of_filter*3])\n",
    "    return layer\n",
    "\n",
    "def create_fc_layer(ip, no_input, no_output):\n",
    "    w_fc= create_wt(shape=[no_input,no_output,])\n",
    "    b_fc= create_bt(no_output)\n",
    "    layer=tf.matmul(ip,w_fc)+b_fc\n",
    "    return(layer)\n",
    "\n",
    "\n",
    "def multirnn(x):\n",
    "    \n",
    "    #embedded_chars = tf.nn.embedding_lookup(w_embedding,x)\n",
    "    #print(embedded_chars)\n",
    "    #x_unstack = tf.unstack(embedded_chars,input_length, 1)\n",
    "    #print(x_unstack)\n",
    "    #x = tf.unstack(x, window_size, 1)\n",
    "    #cell = tf.contrib.rnn.BasicLSTMCell(no_hidden)\n",
    "    #x=tf.transpose(embedded_chars,[1,0,2])\n",
    "    #x=tf.reshape(x,[-1,embed_length])\n",
    "    #x=tf.split(x,input_length,0)\n",
    "    #print(x)\n",
    "    lstm_f_cell=tf.contrib.rnn.BasicLSTMCell(no_hidden,forget_bias=1.0)\n",
    "    lstm_f_cell_d=tf.nn.rnn_cell.DropoutWrapper(lstm_f_cell, output_keep_prob=0.5)\n",
    "    lstm_b_cell=tf.contrib.rnn.BasicLSTMCell(no_hidden,forget_bias=1.0)\n",
    "    lstm_b_cell_d=tf.nn.rnn_cell.DropoutWrapper(lstm_b_cell, output_keep_prob=0.5)\n",
    "    outputs, _, _=tf.contrib.rnn.static_bidirectional_rnn(lstm_f_cell_d,lstm_b_cell_d,x,dtype=tf.float32)\n",
    "    #print('output')\n",
    "    #print(outputs[4])\n",
    "    #outputs, _, _=tf.contrib.rnn.static_bidirectional_rnn(fw_cell,bw_cell,x_unstack,dtype=tf.float32)\n",
    "    #cell=tf.contrib.rnn.MultiRNNCell([cell] * number_of_layers)\n",
    "    #cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "    #outputs, _ =  tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "    #outputs, _, _=tf.contrib.rnn.stack_bidirectional_rnn(fw_cell, bw_cell, x, dtype=tf.float32)\n",
    "    #print('output size')\n",
    "    #print(outputs[4])\n",
    "    logit= [tf.matmul(output,w) for output in outputs]\n",
    "    #print(logit[4])\n",
    "    return(outputs[4])\n",
    "\n",
    "output_filter1= create_conv(ip= x_shape_conv, no_of_filter= 200, conv_filter_size= filter_size1, no_of_channel=1)\n",
    "#print(output_filter1)\n",
    "output_filter2= create_conv(ip= x_shape_conv, no_of_filter= 200, conv_filter_size= filter_size2, no_of_channel=1)\n",
    "#print(output_filter2)\n",
    "output_filter3= create_conv(ip= x_shape_conv, no_of_filter= 200, conv_filter_size= filter_size3, no_of_channel=1)\n",
    "#print(output_filter3)\n",
    "final_vector= tf.concat([output_filter1,output_filter2,output_filter3],0)\n",
    "print(final_vector)\n",
    "output_conv_layer=tf.reshape(final_vector,[-1,no_of_filter*3])\n",
    "print(output_conv_layer)\n",
    "\n",
    "output_lstm_layer=multirnn(x_shape_rnn)\n",
    "print(output_lstm_layer)\n",
    "output_concat= tf.concat([output_conv_layer, output_lstm_layer],1)\n",
    "print(output_concat)\n",
    "\n",
    "layer_fc=create_fc_layer(output_concat, no_input = 1000, no_output =len(uniq_event_type))\n",
    "print(layer_fc)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=layer_fc))#cost calculation\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "y_pred= tf.nn.softmax(layer_fc, name='y_pred')\n",
    "print(y_pred)\n",
    "correct_prediction=tf.equal(tf.argmax(y_pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "predicted_class=tf.argmax(y_pred,1)\n",
    "print(predicted_class)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14046808\n",
      "0.07862905\n",
      "0.042753864\n",
      "0.025611687\n",
      "0.007695614\n",
      "0.007143285\n",
      "0.0029586945\n",
      "0.001159771\n",
      "0.00073160947\n",
      "0.0007757211\n",
      "0.0006764385\n",
      "0.00048049723\n",
      "0.0015819152\n",
      "0.00026286894\n",
      "0.0007309383\n",
      "0.00027756227\n",
      "0.00021502652\n",
      "0.0018461258\n",
      "0.00015776925\n",
      "0.00031081814\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '../work_space/august_last/bengali_pickle_latest/bengali_trigger_model_30_1.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})\n",
    " \n",
    "    batch_size=1000\n",
    "    #no_of_batch= int(len(x_train)/batch_size)\n",
    "    epoch=20\n",
    "    for r in range(epoch):\n",
    "        ptr=1\n",
    "        bs=0\n",
    "        while ptr*batch_size < len(x_train):\n",
    "            ip,op=x_train[bs:bs+batch_size],y_train[bs:bs+batch_size]\n",
    "            #ip_v, op_v= x_dev[:2*batch_size], y_dev[:2*batch_size]\n",
    "            bs+=batch_size\n",
    "            ip=np.reshape(ip,[batch_size,input_length])\n",
    "            op=np.reshape(op,[batch_size,no_class])\n",
    "            ptr+=1\n",
    "            sess.run(train,feed_dict={x:ip,y:op})\n",
    "        print(sess.run(cost,feed_dict={x:ip,y:op}))\n",
    "        #print(sess.run(accuracy,feed_dict={x:ip_v,y:op_v}))\n",
    "\n",
    "    saver.save(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../work_space/august_last/bengali_pickle_latest/bengali_trigger_model_30_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH = '../work_space/august_last/bengali_pickle_latest/bengali_trigger_model_30_1.ckpt'\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "output_path='../work_space/august_last/bengali_pickle_latest/test_trigger_output_31_8/'\n",
    "path='../work_space/august_last/bengali_pickle_latest/test_fixed_window_pikle'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "    for filename  in os.listdir(path):\n",
    "        fullPath = os.path.join(path, filename)\n",
    "        if os.path.isfile(fullPath):\n",
    "            test_file=open(fullPath,'rb')\n",
    "            test_x=pickle.load(test_file)\n",
    "            test_y=pickle.load(test_file)\n",
    "            test_z=pickle.load(test_file)\n",
    "            word_list=pickle.load(test_file)\n",
    "            x_test=test_x\n",
    "            y_test=test_y\n",
    "            acc=0\n",
    "            ptr=1\n",
    "            bs=0\n",
    "            #test_window_size=len(x_test)\n",
    "            #test_x=np.reshape(x_test,[batch_size,test_window_size,embed_length])\n",
    "            #test_y=np.reshape(y_test,[batch_size,test_window_size,no_class])\n",
    "            #acc=sess.run(accuracy,feed_dict={x:test_x,y:test_y})\n",
    "            tagset=[]\n",
    "            batch_size=1\n",
    "            n_batch = int(math.ceil(len(x_test) / batch_size))\n",
    "            print(n_batch)\n",
    "            while ptr <= n_batch:\n",
    "                if ptr*batch_size < len(x_test):\n",
    "                    ip,op=x_test[bs:bs+batch_size],y_test[bs:bs+batch_size]\n",
    "                else:\n",
    "                    ip,op=x_test[bs:],y_test[bs:]\n",
    "                bs+=batch_size\n",
    "                ip=np.reshape(ip,[batch_size,input_length])\n",
    "                #op=np.reshape(op,[batch_size,no_class])\n",
    "                ptr+=1\n",
    "                pred=sess.run(predicted_class,feed_dict={x:ip})\n",
    "                for p1 in pred:\n",
    "                    tagset.append(p1)\n",
    "                    #print 'p1 = ', p1\n",
    "                    #for p2 in p1:\n",
    "                    #print 'p2 = ', np.asarray(softmax(p2))\n",
    "               \n",
    "                #acc+=sess.run(accuracy,feed_dict={x:ip,y:op})\n",
    "            #print (acc/ptr), len(tagset)\n",
    "            #print(tagset)\n",
    "            f_out=open(output_path+filename.replace('pkl','txt'),'w+')\n",
    "            t=0\n",
    "            while t<len(tagset):\n",
    "                f_out.write(str(vocab[x_test[t][4]])+'   '+str(str(y_test[t])+' '+str(uniq_event_type[tagset[t]])+'\\n'))\n",
    "                t+=1\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq_event_list=[]\n",
    "for tag in uniq_event_type:\n",
    "    tag1=str(tag)\n",
    "    if tag1 not in uniq_event_list:\n",
    "        uniq_event_list.append(tag1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "total_tag_trigger_actual=[]\n",
    "total_tag_trigger_predicted=[]\n",
    "path='../work_space/august_last/bengali_pickle_latest/test_trigger_output_31_8/'\n",
    "for filename  in os.listdir(path):\n",
    "    fullPath = os.path.join(path, filename)\n",
    "    if os.path.isfile(fullPath):\n",
    "        actual_tag=[]\n",
    "        predicted_tag=[]\n",
    "        f1=open(fullPath,'r')\n",
    "        for line in f1:\n",
    "            a=line.split()\n",
    "            actual_tag.append(str(a[1]))\n",
    "            predicted_tag.append(str(a[2]))\n",
    "        total_tag_trigger_actual=total_tag_trigger_actual + actual_tag\n",
    "        total_tag_trigger_predicted=total_tag_trigger_predicted + predicted_tag\n",
    "        f1.close()\n",
    "        \n",
    "total_predicted_trigger_list=[]\n",
    "for k in total_tag_trigger_predicted:\n",
    "    if k=='0':\n",
    "        total_predicted_trigger_list.append('None')\n",
    "    else:\n",
    "        total_predicted_trigger_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_result=open('../work_space/august_last/bengali_pickle_latest/test_trigger_output_31_8/result_analysis.txt','w+')\n",
    "for k in uniq_event_list:\n",
    "    i=0\n",
    "    tp=0\n",
    "    total_count=0\n",
    "    for i in range(len(total_tag_trigger_actual)):\n",
    "        if total_tag_trigger_actual[i]==k:\n",
    "            total_count+=1\n",
    "            if total_predicted_trigger_list[i]==k:\n",
    "                tp+=1\n",
    "    fp=0\n",
    "    i=0\n",
    "    for i in range(len(total_tag_trigger_actual)):\n",
    "        if total_predicted_trigger_list[i]==k:\n",
    "            if total_tag_trigger_actual[i]!=k:\n",
    "                fp+=1\n",
    "    fn=0\n",
    "    i=0\n",
    "    for i in range(len(total_tag_trigger_predicted)):\n",
    "        if total_predicted_trigger_list[i]!=k:\n",
    "            if total_tag_trigger_actual[i]==k:\n",
    "                fn+=1\n",
    "    f_result.write(k+' '+'TP='+str(tp)+' '+' FP='+str(fp)+' '+' FN='+str(fn)+'\\n')\n",
    "f_result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
